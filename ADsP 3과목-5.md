## 분류분석과 예측분석

- 공통점 : 튜플의 특정 속성의 값을 미리 알아 맞히는 것
- 차이점
    - 분류는 튜플의 범주형 속성의 값을 알아 맞히는 것
    - 예측은 튜플의 연속성 속성의 값을 알아 맞히는 것
- 분류 기법 : 로지스틱 회귀분석, 의사결정나무, 베이지안 분류, 인공신경망, 지지도벡터기계, K-최근접이웃(KNN), 규칙기반의 분류와 사례기반추론

## 의사결정나무

- 분류 함수를 의사결정 규칙으로 이뤄진 나무 모양으로 그리는 방법
- 주어진 입력값에 대해 출력값을 예측하는 모형으로 분류나무와 회귀나무 모형이 있음
- 특징
    - 계산 결과가 의사결정나무에 직접 나타나게 되어 분석이 간편함.
    - 분류 정확도가 좋음
    - 계산이 복잡하지 않아 대용량 데이터에서도 빠르게 만들 수 있음.
    - 비정상 잡음 데이터에 대해서도 민감함 없이 분류
    - 한 변수와 상관성이 높은 다른 불필요한 변수가 있어도 크게 영향 받지 않음.
- 분석 과정
    - 성장 → 가지치기 → 타당성 평가 → 해석 및 예측
    - 가지치기 : 너무 큰 나무 모형은 자료를 과대적합하고, 너무 작은 나무 모형은 자료를 과소적합할 위험이 있어서 마디에 속한 자료가 일정 수 이하일 경우, 분할을 정지하고 가지치기 실시
    - 불순도에 따른 분할 측도 : 카이제곱 통계량, 지니지수, 엔트로피 지수
- 분석의 종류
    - CART : 목적변수가 범주형인 경우 지니지수, 연속형인 경우 분산을 이용해 이진분리를 사용
    - C4.5와 C5.0 : 다지분리가 가능하고 범주형 입력 변수의 범주 수만큼 분리 가능(엔트로피 지수 사용)
    - CHAID : 가지치기를 하지 않고 적당한 크기에서 나무모형의 성장을 중지하며 입력변수가 반드시 범주형 변수여야함.(카이제곱 통계량 사용)

## 앙상블 기법

- 주어진 자료로부터 여러 개의 예측모형들을 만든 후 조합하여 하나의 최종 예측 모형을 만드는 방법
- 다중 모델조합 방법이 있음
- 학습 방법의 불안정성을 해결하기 위해 고안된 기법
- 가장 불안정성을 가지는 기법은 의사결정나모, 가장 안정성을 가지는 기법은 1-nearest neighbor
- 기법의 종류
    - bagging : 여러 개의 붓스트랩 자료를 생성하고 각 붓스트랩 자료의 예측모형 결과를 결합하여 결과를 선정
    - boosting : 예측력이 약한 모형들을 결합하여 강한 예측 모형들을 만드는 방법
    - Random Forest : 의사결정나무의 특징인 분산이 크다는 점을 고려하여 배깅과 부스팅보다 더많은 무작위성을 주어 약한 학습기들을 생성한 후 이를 선형 결합하여 최종 학습기를 만드는 방법

## 로지스틱 회귀분석

- 반응변수가 범주형인 경우에 적용되는 회귀분석모형
- 새로운 설명변수가 주어질 때 반응변수의 각 범주에 속할 확률이 얼마인지를 추정하여, 추정확률을 기준치에 따라 분류하는 목적으로 활용
- exp의 의미는 나머지 변수가 주어질 때 x1이 한 단위 증가할 때마다 성공의 **오즈**가 몇배 증가하는지를 나타내는 값
- glm함수를 활용하여 로지스틱 회귀분석을 실행함.

### 인공신경망

- 인간의 뇌를 기반으로 추론하는 모델
- **가중치를 반복적으로 조정**하여 학습한다.
- 인공신경망은 가중치를 초기화 하고 훈련 데이터를 통해 가중치를 갱신하여 학습 함
- 입력 신호에서 여러 신호를 받아 활성화 수준을 계산하여 출력 링크로 신호를 보내는 과정
- 뉴런은 활성화 함수(Activation Funtion)을 사용한다.
    - Sigmoid : 이진 분류에 쓰이는 함수(0~1까지의 확률값을 가짐)
    - SoftMax : 다중 분류에 쓰이는 함수(출력층에서 주로 사용)
    - Relu: Hidden Layer에 자주 쓰이는 활성화 함수(입력값이 0이하는 0, 0이상은 x값을 나타냄)
    - Hyperbolic tangent function(tanh): 시그모이드 함수와 유사하며, 기울기 소실증상이 적어 은닉층에서 많이 사용(-1과 1사이의 값을 가짐)
    - Leaky Relu : 0이하인 값을 0으로안하고 0.001과 같은 매우 작은 수를 반화하도록 함.
- 역사
    1. 퍼셉트론 : 초기 형태의 인공 신경망으로 다수의 입력으로부터 하나의 결과를 내보내는 알고리즘
    2. 역전파 알고리즘 : 비선형성(XOR을 풀지 못하는 한계)을 극복한 다계층 퍼셉트론으로 새로운 인공신경망 모형이 등장.

### 신경망 모형 구축시 고려사항

1. 입력 변수
- 범주형 변수 : 각 범주의 빈도가 일정
- 연속형 변수 : 변수간의 큰 차이가 없을때
1. 전처리
- 범주형 : 1과 0  같이 정량적이고 같은 범위를 갖도록 가변수화 필요
- 연속형 : 입력 변수의 분포가 평균을 중심으로 대칭이면 학습이 잘됨, 변환과 범주화 필요
1. 가중치의 초기값과 다중 최소값 문제
- 역전파 알고리즘을 활요하므로 초기값에 민감함.
- 일반적으로 가중치가 작으면 선형 모델에 가깝고, 값이 크면 비선형 모델에 가까움
- 가중치가 0이면 시그모이드 함수는 선형이 되고 신경망 모형은 선형모형이 된다.
1. 학습 모드
- 온라인 학습 모드 (Online Learning Mode) : 순차적으로 하나씩 신경망에 투입
- 확률적 학습 모드 (Probabilistic Learning Mode) : 신경망에 투입되는 관측 값이 랜덤
- 배치 학습 모드 (Batch Learning Mode) : 데이터를 한번에 투입
1. 은닉층과 은닉 노드
- 층이 깊어지면 과대적합 발생 가능성 존재
- 층이 얇으면 과소적합 발생 가능성 존재
- 은닉수가 1개이면 매끄러운 함수를 얻을 수 있으므로 한개부터 학습을 시작
- 노드의 수는 큰 값에서 부터 작게 하는 것이 좋다
- 은닉 노드의 수는 적절히 큰 값으로 놓고 가중치를 감소시키며 적용
1. 과대 적합 문제
- 조기 종료와 가중치 감소 기법으로 해결
- 신경망에서는 많은 가중치를 추정해야 하므로 과대적합 문제가 빈번하다.
